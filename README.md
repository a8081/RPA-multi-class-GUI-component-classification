# RPA-multi-class-GUI-component-classification
Los datos que se encuentran disponibles en este repositorio son datos obtenidos tras el preprocesamiento de un subconjunto del dataset ReDraw presente en la plataforma Zenodo, desarrollado por Moran, Kevin William &amp; Mary Bernal-Cardenas, Carlos; Curcio, Michael; Bonett, Richard; Poshyvanyk, Denys.

Encontraremos en este repositorio varios ficheros:

- Deteccion componentes GUI: notebook con todo lo necesario para detectar y extraer como recortes a partir de una captura de pantalla, todos los componentes de interfaz gráfica (GUI) presentes en ella, como por ejemplo, botones, checkbox o imágenes.
- model.h5: archivo con los pesos de la red neuronal convolucional entrenada en formato HDF5
- model.json: archivo con el modelo serializado en formato JSON
- preprocessed_50_50_balance.npy: array de numpy con las imágenes preprocesadas (Zero-padding + resizing mediante vecinos más cercanos con filtro antialiasing)
- test_labels_balance.npy: array de numpy con las etiquetas correspondientes a las imágenes almacenadas en preprocessed_50_50_balance, almacenadas en el mismo orden, es decir, la imagen de la posición 1 del array tendrá como etiqueta la almacenada en la posición 1 del array test_labels_balance.
- Transfer_learning_ImageNet: notebook con el preprocesamiento, EDA, transfer learning a partir de la deep CNN VGG19 basada en el conjunto de datos de ImageNet, el entrenamiento de la red completa a partir de los datos preprocesados de ReDraw, y un análisis de los resultados.

En este último archivo se puede observar que el entrenamiento con el dataset de ReDraw ha supuesto un reto, que nos ha hecho perder mucho tiempo. Esto se ha debido al alto coste computacional de tratar cerca de 150K imágenes de 1200x1920, ya que para utilizar CNN con Fully Connected Layers, es necesario hacer uso de imágenes con la misma dimensión. Basándonos en el trabajo de [Mahdi Hashemi](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0263-7), decidimos que el Zero-Padding era la mejor opción, ya que era con la que menos información se perdía, y obtuvimos un dataset con todas las imágenes de un tamaño de 1200x1920. El entrenamiento de la red desde cero, con este tipo de datos fue inviable, ya que la potencia de computo de nuestros equipos no lo permitía. Por ello, decidimos (tras varias pruebas como migrar el notebook a Google Colab), realizar un preprocesamiento dirigido a reducir el número de imágenes y su tamaño. De esta manera, nos quedamos con un subconjunto de ReDraw con 15782 imágenes de componentes de interfaz gráfica. La selección la llevamos a cabo tras ver que el groso de las imágenes del dataset se encontraba por debajo del tamaño de 150x150, por lo que seleccionamos a aquellas que estaban por debajo de esa dimensión, específicamente del dataset que en Zenodo destinan para Test. Así de 19K imágenes pasamos a 15K, y una vez seleccionadas estas, redujimos su tamaño haciendo uso de tensorflow (con el método de los vecinos más cercanos y el filtro antialiasing), de 150x150 a 50x50.

Finalmente con este dataset reducido, gracias al asesoramiento de nuestro tutor, decidimos seleccionar un modelo preentrenado y añadirle seis capas más, con el objetivo de hacer mucho más eficiente el entrenamiento del modelo. Y así fue, como resultado, obtuvimos un entrenamiento en 4 horas de la red, obteniendo un accuracy del 55.75%. No es un resultado muy bueno, pero realmente el trabajo que se ha hecho sobre los datos nos permite aumentar fácilmente el rendimiento, simplemente entrenando más épocas con un mayor número de datos.